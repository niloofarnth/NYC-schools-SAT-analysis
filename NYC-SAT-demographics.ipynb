{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = ['ap_2010.csv','class_size.csv','demographics.csv', 'graduation.csv', 'hs_directory.csv', 'sat_results.csv']\n",
    "text_files = ['survey_all.txt', 'survey_d75.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### creating a dictionary of all the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for file in csv_files:\n",
    "    file_name = file.split('.')\n",
    "    data[file_name[0]]= pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### correcting \"survey\" data set and add it to the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the file with its encoding\n",
    "survey_all = pd.read_csv(text_files[0], sep = '\\t', encoding = 'cp1252')\n",
    "survey_d75 = pd.read_csv(text_files[1], sep = '\\t', encoding = 'cp1252')\n",
    "\n",
    "#merging two data set to one\n",
    "survey = pd.concat([survey_all, survey_d75], axis = 0)\n",
    "\n",
    "#add survey to the data dictionary by choosing the custom and useful columns\n",
    "survey['DBN'] = survey['dbn']\n",
    "custom_columns = [\"DBN\", \"rr_s\", \"rr_t\", \"rr_p\", \"N_s\", \"N_t\", \"N_p\", \"saf_p_11\", \"com_p_11\", \"eng_p_11\", \"aca_p_11\", \"saf_t_11\", \"com_t_11\", \"eng_t_11\", \"aca_t_11\", \"saf_s_11\", \"com_s_11\", \"eng_s_11\", \"aca_s_11\", \"saf_tot_11\", \"com_tot_11\", \"eng_tot_11\", \"aca_tot_11\"]\n",
    "survey = survey[custom_columns].copy()\n",
    "data['survey'] = survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### correcting \"class_size\" data set by adding DBN column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to add a zero to left side of the numbers in CSD column\n",
    "def correct_CSD(column):\n",
    "    CSD = str(column).zfill(2) \n",
    "    column = CSD\n",
    "    return column\n",
    "data['class_size']['CSD'] = data['class_size']['CSD'].apply(correct_CSD)\n",
    "\n",
    "#create DBN column\n",
    "data['class_size']['DBN'] = data['class_size']['CSD'] + data['class_size']['SCHOOL CODE']\n",
    "\n",
    "#move DBN column to the front\n",
    "cols = list(data['class_size'])\n",
    "cols.insert(0, cols.pop(cols.index('DBN')))\n",
    "data['class_size'] = data['class_size'].loc[:, cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### correcting \"hs_directory\" data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hs_directory'].rename(columns = {'dbn':'DBN'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### correcting \"ap_2010\" data types to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data['ap_2010'].columns[2:]:\n",
    "    data['ap_2010'][column] = pd.to_numeric(data['ap_2010'][column], errors = 'coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### extracting and adding latitude and longitude columns to \"hs_directory\" for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract the latitude and longitude from column \"Location 1\"\n",
    "def add_coordinates (column):\n",
    "    if column == \"NaN\":\n",
    "        return None\n",
    "    else:\n",
    "        location = re.findall('\\(.+\\)',column)\n",
    "        location = location[0].split(',')\n",
    "        latitude = float(location[0][1:])\n",
    "        longitude = float(location[1][:-1])\n",
    "        coordinates = [latitude, longitude]\n",
    "        return coordinates\n",
    "\n",
    "#apply the function and assign the results to coordinates and create two list from it to add to the main dataframe    \n",
    "coordinates = data['hs_directory']['Location 1'].apply(add_coordinates)\n",
    "latitude = []\n",
    "longitude = []\n",
    "for cor in coordinates:\n",
    "    latitude.append(cor[0])\n",
    "    longitude.append(cor[1])\n",
    "\n",
    "#add the new columns to DataFrame\n",
    "data['hs_directory']['latitude'] = latitude\n",
    "data['hs_directory']['longitude'] = longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### condensing the 'class_size' by deleting the rows that are not useful i.e. not for high school students or "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding all the possible values for 'GRADE' column\n",
    "data['class_size']['GRADE '].unique()\n",
    "#we just want the '09-12' for high school students, so delete the other rows if the GRADE is not '09-12'\n",
    "data['class_size'] = data['class_size'][data['class_size']['GRADE '] == '09-12']\n",
    "\n",
    "#finding the possible values for Program type and their number of repetition\n",
    "data['class_size']['PROGRAM TYPE'].unique()\n",
    "        \n",
    "#GEN ED is the most popular TYPE so delete the rest\n",
    "data['class_size'] = data['class_size'][data['class_size']['PROGRAM TYPE'] == 'GEN ED']\n",
    "\n",
    "#grouping and aggregating based on the DBN and average of other columns\n",
    "data['class_size'] =  data['class_size'].groupby('DBN', as_index = False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### condensing the 'demographics' by deleting the rows other than the schoolyear of 20112012 which is the same as the sat results year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['demographics'] = data['demographics'][data['demographics']['schoolyear'] == 20112012]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### condensing the 'graduation' by keeping only Total Cohort and  2006 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['graduation'] = data['graduation'][data['graduation']['Demographic']== 'Total Cohort'];\n",
    "data['graduation'] = data['graduation'][data['graduation']['Cohort']== '2006'];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### condensing the 'ap_2010' by deleting one row that was excess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04M610 2\n"
     ]
    }
   ],
   "source": [
    "#find the DBN that is repeated more than one row\n",
    "for (index, value) in (data['ap_2010']['DBN'].value_counts()).iteritems():\n",
    "    if value >1:\n",
    "        print(index, value)\n",
    "        \n",
    "#check those rows        \n",
    "data['ap_2010'][data['ap_2010']['DBN'] == '04M610']\n",
    "\n",
    "#delete the row that was extra by knowing its index\n",
    "data['ap_2010'] = data['ap_2010'].drop(52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### summing up all the SAT sections to \"sat_score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sat_results']['SAT Critical Reading Avg. Score'] = pd.to_numeric(data['sat_results']['SAT Critical Reading Avg. Score'],errors = 'coerce')\n",
    "data['sat_results']['SAT Math Avg. Score'] = pd.to_numeric(data['sat_results']['SAT Math Avg. Score'],errors = 'coerce')\n",
    "data['sat_results']['SAT Writing Avg. Score'] = pd.to_numeric(data['sat_results']['SAT Writing Avg. Score'],errors = 'coerce')\n",
    "data['sat_results']['sat_score'] = data['sat_results']['SAT Critical Reading Avg. Score'] \\\n",
    "                                 + data['sat_results']['SAT Math Avg. Score'] \\\n",
    "                                 + data['sat_results']['SAT Writing Avg. Score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### merging all the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = data['sat_results']\n",
    "combined = combined.merge(data['ap_2010'], how = 'left', on = 'DBN')\n",
    "combined = combined.merge(data['graduation'], how = 'left', on = 'DBN')\n",
    "combined = combined.merge(data['class_size'], how = 'inner', on = 'DBN')\n",
    "combined = combined.merge(data['demographics'], how = 'inner', on = 'DBN')\n",
    "combined = combined.merge(data['survey'], how = 'inner', on = 'DBN')\n",
    "combined = combined.merge(data['hs_directory'], how = 'inner', on = 'DBN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### filling the NaN values, and droping two repeated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = combined.fillna(combined.mean())\n",
    "combined = combined.fillna(0)\n",
    "combined.drop('SchoolName', axis = 1, inplace = True)\n",
    "combined.drop('School Name', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### add school_dist column for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['school_dist'] = combined['DBN'].apply(lambda x : x[0:2] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
